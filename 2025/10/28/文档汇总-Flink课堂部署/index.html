<!DOCTYPE html><html lang="zh-Cn" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/fluid-icon.png"><link rel="icon" href="/img/fluid-icon.png"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="John Doe"><meta name="keywords" content=""><meta name="description" content="部署方式，运行模式"><meta property="og:type" content="article"><meta property="og:title" content="文档汇总-Flink课堂部署"><meta property="og:url" content="https://codesave666.github.io/2025/10/28/%E6%96%87%E6%A1%A3%E6%B1%87%E6%80%BB-Flink%E8%AF%BE%E5%A0%82%E9%83%A8%E7%BD%B2/index.html"><meta property="og:site_name" content="Hexo"><meta property="og:description" content="部署方式，运行模式"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://pic2.zhimg.com/v2-9785152f4413d7d403d3da0ea08c3fab_720w.jpg?source=172ae18b"><meta property="article:published_time" content="2025-10-27T16:00:00.000Z"><meta property="article:modified_time" content="2025-10-28T09:30:05.294Z"><meta property="article:author" content="John Doe"><meta property="article:tag" content="Flink"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://pic2.zhimg.com/v2-9785152f4413d7d403d3da0ea08c3fab_720w.jpg?source=172ae18b"><title>文档汇总-Flink课堂部署 - Hexo</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css"><link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var CONFIG={hostname:"codesave666.github.io",root:"/",version:"1.9.8",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!1,follow_dnt:!0,baidu:null,google:{measurement_id:null},tencent:{sid:null,cid:null},leancloud:{app_id:null,app_key:null,server_url:null,path:"window.location.pathname",ignore_local:!1},umami:{src:null,website_id:null,domains:null,start_time:"2024-01-01T00:00:00.000Z",token:null,api_server:null}},search_path:"/local-search.xml",include_content_in_search:!0};if(CONFIG.web_analytics.follow_dnt){var dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack;Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on"))}</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><meta name="generator" content="Hexo 7.3.0"></head><body><header><div class="header-inner" style="height:70vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>Fluid</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/" target="_self"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/" target="_self"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/" target="_self"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/" target="_self"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/" target="_self"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/index.png) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="文档汇总-Flink课堂部署"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2025-10-28 00:00" pubdate>2025年10月28日 凌晨</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 6.9k wörter </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 58 minuten</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 id="seo-header">文档汇总-Flink课堂部署</h1><div class="markdown-body"><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;准备工作"></a>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;准备工作</h2><h3 id="1-创建hadoop用户"><a href="#1-创建hadoop用户" class="headerlink" title="1.创建hadoop用户"></a>1.创建hadoop用户</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> useradd -m hadoop -s /bin/bash<br></code></pre></td></tr></table></figure><p><u>设置密码</u></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> passwd hadoop<br></code></pre></td></tr></table></figure><p><u>添加管理员权限</u></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> adduser hadoop <span class="hljs-built_in">sudo</span><br></code></pre></td></tr></table></figure><p>最后，退出登陆使用新创建的hadoop用户登陆。</p><h3 id="2-更新APT（无所谓）"><a href="#2-更新APT（无所谓）" class="headerlink" title="2.更新APT（无所谓）"></a>2.更新APT（无所谓）</h3><p>我习惯vim编辑器：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> apt install vim<br></code></pre></td></tr></table></figure><ol><li>在设置中把这个旧的源关闭：</li></ol><p><img src="/./assets/image-20250903160938614.png" srcset="/img/loading.gif" lazyload alt="image-20250903160938614"></p><ol start="2"><li>打开源的配置文件</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> gedit /etc/apt/sources.list<br></code></pre></td></tr></table></figure><ol start="3"><li><p>添加为阿里云的Ubuntu源，根据ubuntu版本填内容:</p><p><a target="_blank" rel="noopener" href="https://developer.aliyun.com/mirror/ubuntu/?spm=a2c6h.25603864.0.0.3ebb28b9DVTb8u">https://developer.aliyun.com/mirror/ubuntu/?spm=a2c6h.25603864.0.0.3ebb28b9DVTb8u</a></p><p>ubuntu 16.04 LTS (xenial) (EOL) 配置如下:</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs awk">deb https:<span class="hljs-regexp">//mi</span>rrors.aliyun.com<span class="hljs-regexp">/ubuntu/</span> xenial main<br>deb-src https:<span class="hljs-regexp">//mi</span>rrors.aliyun.com<span class="hljs-regexp">/ubuntu/</span> xenial main<br><br>deb https:<span class="hljs-regexp">//mi</span>rrors.aliyun.com<span class="hljs-regexp">/ubuntu/</span> xenial-updates main<br>deb-src https:<span class="hljs-regexp">//mi</span>rrors.aliyun.com<span class="hljs-regexp">/ubuntu/</span> xenial-updates main<br><br>deb https:<span class="hljs-regexp">//mi</span>rrors.aliyun.com<span class="hljs-regexp">/ubuntu/</span> xenial universe<br>deb-src https:<span class="hljs-regexp">//mi</span>rrors.aliyun.com<span class="hljs-regexp">/ubuntu/</span> xenial universe<br>deb https:<span class="hljs-regexp">//mi</span>rrors.aliyun.com<span class="hljs-regexp">/ubuntu/</span> xenial-updates universe<br>deb-src https:<span class="hljs-regexp">//mi</span>rrors.aliyun.com<span class="hljs-regexp">/ubuntu/</span> xenial-updates universe<br><br>deb https:<span class="hljs-regexp">//mi</span>rrors.aliyun.com<span class="hljs-regexp">/ubuntu/</span> xenial-security main<br>deb-src https:<span class="hljs-regexp">//mi</span>rrors.aliyun.com<span class="hljs-regexp">/ubuntu/</span> xenial-security main<br>deb https:<span class="hljs-regexp">//mi</span>rrors.aliyun.com<span class="hljs-regexp">/ubuntu/</span> xenial-security universe<br>deb-src https:<span class="hljs-regexp">//mi</span>rrors.aliyun.com<span class="hljs-regexp">/ubuntu/</span> xenial-security universe<br><br></code></pre></td></tr></table></figure></li><li><p>最后更新即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> apt update &amp;&amp; <span class="hljs-built_in">sudo</span> apt upgrade<br></code></pre></td></tr></table></figure></li></ol><h3 id="3-安装SSH"><a href="#3-安装SSH" class="headerlink" title="3.安装SSH"></a>3.安装SSH</h3><p><u>安装ssh服务端（Ubuntu默认已安装ssh客户端）</u></p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">sudo apt-<span class="hljs-keyword">get</span> install openssh-<span class="hljs-keyword">server</span><br></code></pre></td></tr></table></figure><p><u>与本机进行ssh连接</u></p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">ssh localhost</span><br></code></pre></td></tr></table></figure><p>exit可以退出连接。</p><p><u>免密ssh登陆</u></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> ~/.ssh/ <br>ssh-keygen -t rsa <br><span class="hljs-built_in">cat</span> ./id_rsa.pub &gt;&gt; ./authorized_keys <br></code></pre></td></tr></table></figure><h3 id="4-安装Java环境"><a href="#4-安装Java环境" class="headerlink" title="4.安装Java环境"></a>4.安装Java环境</h3><ol><li>上传解压</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /usr/lib<br><span class="hljs-built_in">sudo</span> <span class="hljs-built_in">mkdir</span> jvm<br><span class="hljs-built_in">cd</span> ~<br>上传压缩包jdk-8u371-linux-x64.tar.gz<br><span class="hljs-built_in">sudo</span> tar -zxvf ./jdk-8u371-linux-x64.tar.gz -C /usr/lib/jvm<br><br></code></pre></td></tr></table></figure><ol start="2"><li><p>设置环境变量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">vim ~/.bashrc<br></code></pre></td></tr></table></figure><p>内容：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-built_in">export</span> <span class="hljs-attribute">JAVA_HOME</span>=/usr/lib/jvm/jdk1.8.0_371<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">JRE_HOME</span>=<span class="hljs-variable">$&#123;JAVA_HOME&#125;</span>/jre<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">CLASSPATH</span>=.:$&#123;JAVA_HOME&#125;/lib:<span class="hljs-variable">$&#123;JRE_HOME&#125;</span>/lib<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">PATH</span>=<span class="hljs-variable">$&#123;JAVA_HOME&#125;</span>/bin:$PATH<br><br></code></pre></td></tr></table></figure></li><li><p>立即生效</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">source</span> ~/.bashrc<br></code></pre></td></tr></table></figure></li><li><p>检查</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs applescript">java -<span class="hljs-built_in">version</span><br></code></pre></td></tr></table></figure></li></ol><h3 id="5-安装配置Hadoop（9870-8088）"><a href="#5-安装配置Hadoop（9870-8088）" class="headerlink" title="5.安装配置Hadoop（9870,8088）"></a>5.安装配置Hadoop（9870,8088）</h3><h4 id="1-安装"><a href="#1-安装" class="headerlink" title="1.安装"></a>1.安装</h4><p>地址：<a target="_blank" rel="noopener" href="https://archive.apache.org/dist/hadoop/core/hadoop-3.3.5/">https://archive.apache.org/dist/hadoop/core/hadoop-3.3.5/</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> tar -zxvf ~/hadoop-3.3.5.tar.gz -C /usr/local <br><span class="hljs-built_in">cd</span> /usr/local/<br><span class="hljs-built_in">sudo</span> <span class="hljs-built_in">mv</span> ./hadoop-3.3.5/ ./hadoop <br><span class="hljs-built_in">sudo</span> <span class="hljs-built_in">chown</span> -R hadoop:hadoop ./hadoop <br><span class="hljs-built_in">cd</span> /usr/local/hadoop<br>./bin/hadoop version<br><br></code></pre></td></tr></table></figure><h4 id="2-本地模式（默认）"><a href="#2-本地模式（默认）" class="headerlink" title="2.本地模式（默认）"></a>2.本地模式（默认）</h4><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs awk">cd <span class="hljs-regexp">/usr/</span>local/hadoop<br>.<span class="hljs-regexp">/bin/</span>hadoop jar .<span class="hljs-regexp">/share/</span>hadoop<span class="hljs-regexp">/mapreduce/</span>hadoop-mapreduce-examples-<span class="hljs-number">3.3</span>.<span class="hljs-number">5</span>.jar<br></code></pre></td></tr></table></figure><p><strong>grep例子</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /usr/local/hadoop<br><span class="hljs-built_in">mkdir</span> input<br><span class="hljs-built_in">cp</span> ./etc/hadoop/*.xml ./input <br>./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar grep ./input ./output <span class="hljs-string">&#x27;dfs[a-z.]+&#x27;</span><br><span class="hljs-built_in">cat</span> ./output/<br></code></pre></td></tr></table></figure><blockquote><p>在“&#x2F;usr&#x2F;local&#x2F;hadoop”目录下创建一个文件夹input，并复制一些文件到该文件夹下，然后，运行grep程序，将 input文件夹中的所有文件作为grep的输入，让grep程序从所有文件中筛选出符合正则表达式“dfs[a-z.]+”的单词，并统计单词出现的次数，最后，把统计结果输出到“&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;output”文件夹.</p></blockquote><blockquote><p>注意：Hadoop默认不会覆盖结果文件，因此，再次运行上面实例会提示出错，解决：rm -r .&#x2F;output</p></blockquote><h4 id="3-伪分布式模式"><a href="#3-伪分布式模式" class="headerlink" title="3.伪分布式模式"></a>3.伪分布式模式</h4><h5 id="3-1-修改配置文件"><a href="#3-1-修改配置文件" class="headerlink" title="3.1 修改配置文件"></a>3.1 修改配置文件</h5><p>core-site.xml</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">vim <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/hadoop/</span>etc<span class="hljs-regexp">/hadoop/</span>core-site.xml<br></code></pre></td></tr></table></figure><p>覆盖内容：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hadoop.tmp.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>file:/usr/local/hadoop/tmp<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">description</span>&gt;</span>Abase for other temporary directories.<span class="hljs-tag">&lt;/<span class="hljs-name">description</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>fs.defaultFS<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://localhost:9000<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure><p>hdfs-site.xml</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">vim <span class="hljs-regexp">/usr/</span>local<span class="hljs-regexp">/hadoop/</span>etc<span class="hljs-regexp">/hadoop/</span>hdfs-site.xml<br></code></pre></td></tr></table></figure><p>覆盖内容：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.replication<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>1<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.name.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>file:/usr/local/hadoop/tmp/dfs/name<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.datanode.data.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>file:/usr/local/hadoop/tmp/dfs/data<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure><h5 id="3-2-节点格式化"><a href="#3-2-节点格式化" class="headerlink" title="3.2 节点格式化"></a>3.2 节点格式化</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /usr/local/hadoop<br>./bin/hdfs namenode -format<br></code></pre></td></tr></table></figure><h5 id="3-3-启动Hadoop"><a href="#3-3-启动Hadoop" class="headerlink" title="3.3 启动Hadoop"></a>3.3 启动Hadoop</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /usr/local/hadoop<br>./sbin/start-dfs.sh <br></code></pre></td></tr></table></figure><h5 id="3-5-测试"><a href="#3-5-测试" class="headerlink" title="3.5 测试"></a>3.5 测试</h5><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">jps</span><br></code></pre></td></tr></table></figure><p>则显示NameNode、DataNode和SecondaryNameNode</p><h5 id="3-6-web测试"><a href="#3-6-web测试" class="headerlink" title="3.6 web测试"></a>3.6 web测试</h5><p><a target="_blank" rel="noopener" href="http://localhost:9870/">http://localhost:9870</a></p><h5 id="3-7-案例"><a href="#3-7-案例" class="headerlink" title="3.7 案例"></a>3.7 案例</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /usr/local/hadoop<br>./bin/hdfs dfs -<span class="hljs-built_in">mkdir</span> -p /user/hadoop<br><span class="hljs-built_in">cd</span> /usr/local/hadoop<br>./bin/hdfs dfs -<span class="hljs-built_in">mkdir</span> input<br>./bin/hdfs dfs -put ./etc/hadoop/*.xml input<br>./bin/hdfs dfs -<span class="hljs-built_in">ls</span> input<br>./bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.5.jar grep input output <span class="hljs-string">&#x27;dfs[a-z.]+&#x27;</span><br>./bin/hdfs dfs -<span class="hljs-built_in">cat</span> output/*<br></code></pre></td></tr></table></figure><p>再次运行得删除生成文件夹</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">./bin/hdfs dfs -<span class="hljs-built_in">rm</span> -r output<br></code></pre></td></tr></table></figure><h5 id="3-8-配置PATH变量"><a href="#3-8-配置PATH变量" class="headerlink" title="3.8 配置PATH变量"></a>3.8 配置PATH变量</h5><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs elixir">vim ~/.bashrc<br>export <span class="hljs-title class_">PATH</span>=<span class="hljs-variable">$PATH</span><span class="hljs-symbol">:/user/local/hadoop/sbin</span><span class="hljs-symbol">:/user/local/hadoop/bin</span><br>source ~/.bashrc<br></code></pre></td></tr></table></figure><h4 id="4-分布式模式（重点）"><a href="#4-分布式模式（重点）" class="headerlink" title="4.分布式模式（重点）"></a>4.分布式模式（重点）</h4><h5 id="4-1-网络"><a href="#4-1-网络" class="headerlink" title="4.1 网络"></a>4.1 网络</h5><blockquote><p>netstat -nr可以看网关，改完ip要重启网络。</p></blockquote><p><img src="/./assets/image-20250909225416870.png" srcset="/img/loading.gif" lazyload alt="image-20250909225416870"></p><p>修改网络配置文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> vim /etc/hosts<br></code></pre></td></tr></table></figure><p>增加映射关系：</p><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs accesslog"><span class="hljs-number">192.168.203.100</span>  hadoop01<br><span class="hljs-number">192.168.203.101</span>  hadoop02<br><span class="hljs-number">192.168.203.102</span>  hadoop03<br></code></pre></td></tr></table></figure><blockquote><p>另外两台一样操作</p></blockquote><h5 id="4-2-SSH无密码登录节点"><a href="#4-2-SSH无密码登录节点" class="headerlink" title="4.2 SSH无密码登录节点"></a>4.2 SSH无密码登录节点</h5><p>安装：</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">sudo apt-<span class="hljs-keyword">get</span> install openssh-<span class="hljs-keyword">server</span><br></code></pre></td></tr></table></figure><p>密钥认证：</p><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs elixir">cd ~/.ssh <br>rm ./id_rsa* <br>ssh-keygen -t rsa <br>cat ./id_rsa.pub &gt;&gt; ./authorized_keys<br><br>scp ~/.ssh/id_rsa.pub hadoop<span class="hljs-variable">@hadoop02</span><span class="hljs-symbol">:/home/hadoop/</span><br>scp ~/.ssh/id_rsa.pub hadoop<span class="hljs-variable">@hadoop03</span><span class="hljs-symbol">:/home/hadoop/</span><br></code></pre></td></tr></table></figure><blockquote><p>在节点hadoop02和hadoop03上分别执行如下命令将SSH公匙加入授权：</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">mkdir</span> ~/.ssh <span class="hljs-comment">#如果不存在该文件夹需先创建，若已存在，则忽略本命令</span><br><span class="hljs-built_in">cat</span> ~/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys<br><span class="hljs-built_in">rm</span> ~/id_rsa.pub <span class="hljs-comment">#用完以后就可以删掉</span><br><br></code></pre></td></tr></table></figure><p>测试</p><figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs smali">ssh hadoop02<br>ssh hadoop03<br></code></pre></td></tr></table></figure><h5 id="4-3-安装hadoop文件"><a href="#4-3-安装hadoop文件" class="headerlink" title="4.3 安装hadoop文件"></a>4.3 安装hadoop文件</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> tar -zxvf ~/hadoop-3.3.5.tar.gz -C /usr/local <br><span class="hljs-built_in">cd</span> /usr/local/<br><span class="hljs-built_in">sudo</span> <span class="hljs-built_in">mv</span> ./hadoop-3.3.5/ ./hadoop <br><span class="hljs-built_in">sudo</span> <span class="hljs-built_in">chown</span> -R hadoop:hadoop ./hadoop <br></code></pre></td></tr></table></figure><h5 id="4-4-配置PATH变量"><a href="#4-4-配置PATH变量" class="headerlink" title="4.4 配置PATH变量"></a>4.4 配置PATH变量</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">vim ~/.bashrc<br></code></pre></td></tr></table></figure><p>加内容：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-built_in">export</span> <span class="hljs-attribute">HADOOP_HOME</span>=/usr/local/hadoop<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">PATH</span>=<span class="hljs-variable">$PATH</span>:$HADOOP_HOME/bin:$HADOOP_HOME/sbin<br></code></pre></td></tr></table></figure><h5 id="4-5-配置文件"><a href="#4-5-配置文件" class="headerlink" title="4.5 配置文件"></a>4.5 配置文件</h5><blockquote><p>位置：&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;etc&#x2F;hadoop</p><p>文件：workers、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml共5个文件</p><p>hadoop-env.sh：运行 Hadoop 要用的环境变量。</p><p>core-site.xml：核心配置项，包括 HDFS、MapReduce 和 YARN 常用的 I&#x2F;O 设置等。</p><p>hdfs-site.xml：HDFS相关进程的配置项，包括 NameNode、SecondaryNameNode、DataNode等。</p><p>yarn-site.xml：YARN 相关进程的配置项，包括 ResourceManager、NodeManager 等。</p><p>mapred-site.xml：MapReduce 相关进程的配置项。</p><p>slaves：从节点配置文件，通常每行 1 个从节点主机名。</p><p>log4j.properties：系统日志、NameNode 审计日志、JVM 进程日志的配置项。</p></blockquote><h6 id="4-5-1-workers"><a href="#4-5-1-workers" class="headerlink" title="4.5.1 workers"></a>4.5.1 workers</h6><p>覆盖内容为：</p><figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs smali">hadoop01<br>hadoop02<br>hadoop03<br></code></pre></td></tr></table></figure><h6 id="4-5-2-core-site-xml"><a href="#4-5-2-core-site-xml" class="headerlink" title="4.5.2 core-site.xml"></a>4.5.2 core-site.xml</h6><p>覆盖内容为：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>fs.defaultFS<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://hadoop01:9000<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>        <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hadoop.tmp.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>file:/usr/local/hadoop/tmp<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">description</span>&gt;</span>Abase for other temporary directories.<span class="hljs-tag">&lt;/<span class="hljs-name">description</span>&gt;</span><br>        <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure><h6 id="4-5-3-hdfs-site-xml"><a href="#4-5-3-hdfs-site-xml" class="headerlink" title="4.5.3 hdfs-site.xml"></a>4.5.3 hdfs-site.xml</h6><p>一份数据保存三份副本</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hadoop03:50090<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>        <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.replication<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>3<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>        <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.name.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>file:/usr/local/hadoop/tmp/dfs/name<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>        <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.datanode.data.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>file:/usr/local/hadoop/tmp/dfs/data<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>        <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure><h6 id="4-5-4-mapred-site-xml"><a href="#4-5-4-mapred-site-xml" class="headerlink" title="4.5.4 mapred-site.xml"></a>4.5.4 mapred-site.xml</h6><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>mapreduce.framework.name<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>yarn<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>        <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hadoop01:10020<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>        <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hadoop01:19888<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>        <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.app.mapreduce.am.env<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>HADOOP_MAPRED_HOME=/usr/local/hadoop<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>mapreduce.map.env<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>HADOOP_MAPRED_HOME=/usr/local/hadoop<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>mapreduce.reduce.env<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>	<span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>HADOOP_MAPRED_HOME=/usr/local/hadoop<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span> <br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure><h6 id="4-5-5-yarn-site-xml"><a href="#4-5-5-yarn-site-xml" class="headerlink" title="4.5.5 yarn-site.xml"></a>4.5.5 yarn-site.xml</h6><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hadoop01<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>        <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>mapreduce_shuffle<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>        <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure><h6 id="4-5-6-scp分发机器"><a href="#4-5-6-scp分发机器" class="headerlink" title="4.5.6 scp分发机器"></a>4.5.6 scp分发机器</h6><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /usr/local/hadoop<br><span class="hljs-built_in">sudo</span> <span class="hljs-built_in">rm</span> -r ./tmp<br><span class="hljs-built_in">sudo</span> <span class="hljs-built_in">rm</span> -r ./logs/*<br><span class="hljs-built_in">cd</span> /usr/local<br>tar -zcf ~/hadoop.master.tar.gz ./hadoop   <br><span class="hljs-built_in">cd</span> ~<br>scp ./hadoop.master.tar.gz hadoop02:/home/hadoop<br>scp ./hadoop.master.tar.gz hadoop03:/home/hadoop<br></code></pre></td></tr></table></figure><blockquote><p>然后在hadoop02和hadoop03节点上分别执行如下命令：</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> ~<br><span class="hljs-built_in">sudo</span> <span class="hljs-built_in">rm</span> -r /usr/local/hadoop<br><span class="hljs-built_in">sudo</span> tar -zxf ~/hadoop.master.tar.gz -C /usr/local<br><span class="hljs-built_in">sudo</span> <span class="hljs-built_in">chown</span> -R hadoop /usr/local/hadoop<br><br></code></pre></td></tr></table></figure><h5 id="4-5-启动"><a href="#4-5-启动" class="headerlink" title="4.5 启动"></a>4.5 启动</h5><p>第一次启动需要格式化：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /usr/local/hadoop<br>./bin/hdfs namenode -format<br></code></pre></td></tr></table></figure><p>标准启动：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /usr/local/hadoop<br>./sbin/start-dfs.sh<br>./sbin/start-yarn.sh<br>./sbin/mr-jobhistory-daemon.sh start historyserver<br></code></pre></td></tr></table></figure><p>输入jps可查看进程：</p><p>hadoop01</p><p><img src="/./assets/image-20250909231416066.png" srcset="/img/loading.gif" lazyload alt="image-20250909231416066"></p><p>hadoop02</p><p><img src="/./assets/image-20250909231449912.png" srcset="/img/loading.gif" lazyload alt="image-20250909231449912"></p><p>hadoop03</p><p><img src="/./assets/image-20250909231510028.png" srcset="/img/loading.gif" lazyload alt="image-20250909231510028"></p><h5 id="4-6-检查数据节点"><a href="#4-6-检查数据节点" class="headerlink" title="4.6 检查数据节点"></a>4.6 检查数据节点</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /usr/local/hadoop<br>./bin/hdfs dfsadmin -report<br></code></pre></td></tr></table></figure><h4 id="5-WordCount测试样例"><a href="#5-WordCount测试样例" class="headerlink" title="5.WordCount测试样例"></a>5.WordCount测试样例</h4><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">hadoop jar <span class="hljs-variable">$HADOOP_HOME</span><span class="hljs-regexp">/share/</span>hadoop<span class="hljs-regexp">/mapreduce/</span>hadoop-mapreduce-examples-*.jar wordcount words.txt output<br></code></pre></td></tr></table></figure><blockquote><p>words.txt：需要统计的文件，要先上传到hdfs哦</p><p>output：输出的结果，不能创建，自动生成的文件夹</p></blockquote><p>查看的话就是</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hadoop fs -<span class="hljs-built_in">cat</span> /output/part*<br></code></pre></td></tr></table></figure><p>如果再次运行需要把output删掉。。。</p><h3 id="6-安装Mysql"><a href="#6-安装Mysql" class="headerlink" title="6.安装Mysql"></a>6.安装Mysql</h3><h4 id="6-1-安装"><a href="#6-1-安装" class="headerlink" title="6.1 安装"></a>6.1 安装</h4><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">sudo apt-<span class="hljs-keyword">get</span> install mysql-<span class="hljs-keyword">server</span><br></code></pre></td></tr></table></figure><h4 id="6-2-启动"><a href="#6-2-启动" class="headerlink" title="6.2 启动"></a>6.2 启动</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs sql">service mysql <span class="hljs-keyword">start</span><br>mysql <span class="hljs-operator">-</span>u root <span class="hljs-operator">-</span>p<br><span class="hljs-keyword">show</span> variables <span class="hljs-keyword">like</span> ‘<span class="hljs-type">char</span><span class="hljs-operator">%</span>’;<br><span class="hljs-keyword">set</span> character_set_server<span class="hljs-operator">=</span>utf8;<br></code></pre></td></tr></table></figure><blockquote><p>查看状态：sudo netstat -tap | grep mysql</p><p>systemctl status mysql.service</p></blockquote><h4 id="6-3-配置文件"><a href="#6-3-配置文件" class="headerlink" title="6.3 配置文件"></a>6.3 配置文件</h4><p>修改编码格式：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs vim">sudo <span class="hljs-keyword">vim</span> /etc/mysql/mysql.<span class="hljs-keyword">conf</span>.d/mysqld.<span class="hljs-keyword">cnf</span><br></code></pre></td></tr></table></figure><p>在[mysqld]下面添加一行：</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">character_set_server</span>=utf8<br></code></pre></td></tr></table></figure><p>重启生效：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> service mysql restart<br></code></pre></td></tr></table></figure><h3 id="7-安装Kafka"><a href="#7-安装Kafka" class="headerlink" title="7.安装Kafka"></a>7.安装Kafka</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> tar -zxvf kafka_2.12-3.5.1.tgz -C /usr/local<br><span class="hljs-built_in">cd</span> /usr/local<br><span class="hljs-built_in">sudo</span> <span class="hljs-built_in">mv</span> kafka_2.12-3.5.1 kafka<br><span class="hljs-built_in">sudo</span> <span class="hljs-built_in">chown</span> -R hadoop ./kafka<br></code></pre></td></tr></table></figure><h5 id="7-1-启动"><a href="#7-1-启动" class="headerlink" title="7.1 启动"></a>7.1 启动</h5><p>一个终端启动Zookeeper服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span>  /usr/local/kafka<br>./bin/zookeeper-server-start.sh  config/zookeeper.properties<br></code></pre></td></tr></table></figure><p>再开一个终端启动Kfaka</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span>  /usr/local/kafka<br>./bin/kafka-server-start.sh  config/server.properties<br></code></pre></td></tr></table></figure><h5 id="7-2-测试"><a href="#7-2-测试" class="headerlink" title="7.2 测试"></a>7.2 测试</h5><p>创建一个自定义名称为“wordsendertest”的Topic：</p><p>打开第三个终端：</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli"><span class="hljs-keyword">cd</span>  <span class="hljs-string">/usr/local/kafka</span><br><span class="hljs-string">./bin/kafka-topics.sh</span>  <span class="hljs-params">--create</span>  <span class="hljs-params">--bootstrap-server</span> localhost<span class="hljs-function">:9092</span> \<br><span class="hljs-params">--replication-factor</span>  1  <span class="hljs-params">--partitions</span>  1  <span class="hljs-params">--topic</span>  wordsendertest<br><br></code></pre></td></tr></table></figure><blockquote><p>这个Topic叫wordsendertest，–bootstrap-server 是 Kafka 客户端用来连接 Kafka 集群的参数。</p><p>9092 是 Kafka Broker 默认的监听端口，–partitions是Topic里面的分区数，–replication-factor是备份的数量，在Kafka集群中使用，由于这里是单机版，所以不用备份</p></blockquote><p>可以用list列出所有创建的Topic，来查看上面创建的Topic是否存在</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli"><span class="hljs-string">./bin/kafka-topics.sh</span> <span class="hljs-params">--bootstrap-server</span> localhost<span class="hljs-function">:9092</span> <span class="hljs-params">--list</span><br></code></pre></td></tr></table></figure><p>这个名称为“wordsendertest”的Topic，就是专门负责采集发送一些单词的。</p><p>下面用生产者（Producer）来产生一些数据，请在当前终端内继续输入下面命令：</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli"><span class="hljs-string">./bin/kafka-console-producer.sh</span>  <span class="hljs-params">--broker-list</span>  localhost<span class="hljs-function">:9092</span> \<br><span class="hljs-params">--topic</span>  wordsendertest<br></code></pre></td></tr></table></figure><p>上面命令执行后，就可以在当前终端（假设名称为“生产者终端”）内用键盘输入一些英文单词，比如可以输入：</p><p>hello hadoop</p><p>hello flink</p><p>这些单词就是数据源，会被Kafka捕捉到以后发送给消费者。现在可以启动一个消费者，来查看刚才生产者产生的数据。请另外打开第四个终端，输入下面命令：</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli"><span class="hljs-keyword">cd</span> <span class="hljs-string">/usr/local/kafka</span><br><span class="hljs-string">./bin/kafka-console-consumer.sh</span>  <span class="hljs-params">--bootstrap-server</span>  localhost<span class="hljs-function">:9092</span>  \<br><span class="hljs-params">--topic</span>  wordsendertest  <span class="hljs-params">--from-beginning</span><br><br></code></pre></td></tr></table></figure><h2 id="Flink（8081）"><a href="#Flink（8081）" class="headerlink" title="&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;Flink（8081）"></a>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;Flink（8081）</h2><h3 id="1-安装部署"><a href="#1-安装部署" class="headerlink" title="1.安装部署"></a>1.安装部署</h3><blockquote><p>下载地址：<a target="_blank" rel="noopener" href="https://archive.apache.org/dist/flink/flink-1.17.0/flink-1.17.0-bin-scala_2.12.tgz">https://archive.apache.org/dist/flink/flink-1.17.0/flink-1.17.0-bin-scala_2.12.tgz</a></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span>  /home/hadoop<br><span class="hljs-built_in">sudo</span>  tar  -zxvf  ~/flink-1.17.0-bin-scala_2.12.tgz -C /usr/local/<br><span class="hljs-built_in">cd</span>  /usr/local<br><span class="hljs-built_in">sudo</span>  <span class="hljs-built_in">mv</span>  ./flink-1.17.0 ./flink<br><span class="hljs-built_in">sudo</span>  <span class="hljs-built_in">chown</span>  -R  hadoop:hadoop  ./flink<br></code></pre></td></tr></table></figure><h3 id="2-配置PATH"><a href="#2-配置PATH" class="headerlink" title="2.配置PATH"></a>2.配置PATH</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">vim ~/.bashrc<br></code></pre></td></tr></table></figure><p>添加：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-built_in">export</span> <span class="hljs-attribute">FLINK_HOME</span>=/usr/local/flink<br><span class="hljs-built_in">export</span> <span class="hljs-attribute">PATH</span>=<span class="hljs-variable">$FLINK_HOME</span>/bin:$PATH<br></code></pre></td></tr></table></figure><p>保持生效：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">source</span> ~/.bashrc<br></code></pre></td></tr></table></figure><h3 id="3-log4j-properties配置文件"><a href="#3-log4j-properties配置文件" class="headerlink" title="3.log4j.properties配置文件"></a>3.log4j.properties配置文件</h3><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stata"><span class="hljs-keyword">cd</span> /usr/<span class="hljs-keyword">local</span>/flink/<span class="hljs-keyword">conf</span><br>vim log4j.properties<br></code></pre></td></tr></table></figure><p>增加如下内容：</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">log4j.appender.console</span>=org.apache.log4j.ConsoleAppender<br><span class="hljs-attr">log4j.appender.console.layout</span>=org.apache.log4j.PatternLayout<br><span class="hljs-attr">log4j.appender.console.layout.ConversionPattern</span>=%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; %-<span class="hljs-number">5</span>p %-<span class="hljs-number">60</span>c %x - %m%n<br><span class="hljs-attr">log4j.rootLogger</span>=<span class="hljs-variable">$&#123;rootLogger.level&#125;</span>, console<br></code></pre></td></tr></table></figure><h3 id="4-单机版启动Flink"><a href="#4-单机版启动Flink" class="headerlink" title="4.单机版启动Flink"></a>4.单机版启动Flink</h3><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stata"><span class="hljs-keyword">cd</span> /usr/<span class="hljs-keyword">local</span>/flink<br>./bin/start-<span class="hljs-keyword">cluster</span>.<span class="hljs-keyword">sh</span><br></code></pre></td></tr></table></figure><blockquote><p>jps</p><p>17942 TaskManagerRunner</p><p>18022 Jps</p><p>17503 StandaloneSessionClusterEntrypoint</p></blockquote><h3 id="5-集群版"><a href="#5-集群版" class="headerlink" title="5.集群版"></a>5.集群版</h3><h4 id="5-1-还需继续配置："><a href="#5-1-还需继续配置：" class="headerlink" title="5.1 还需继续配置："></a>5.1 还需继续配置：</h4><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stata"><span class="hljs-keyword">cd</span>  /usr/<span class="hljs-keyword">local</span>/flink/<span class="hljs-keyword">conf</span><br>vim flink-<span class="hljs-keyword">conf</span>.yaml<br></code></pre></td></tr></table></figure><blockquote><p>在flink-conf.yaml中按照如下内容设置好如下各个配置项（文件中的其他配置项不用修改）：</p><p>jobmanager.rpc.address: hadoop01</p><p>jobmanager.bind-host: 0.0.0.0</p><p>taskmanager.bind-host: 0.0.0.0</p><p>taskmanager.host: hadoop01</p><p>rest.address: hadoop01</p><p>rest.bind-address: 0.0.0.0</p></blockquote><p>或者直接覆盖：</p><blockquote><p>jobmanager.rpc.address: hadoop01</p><p>jobmanager.rpc.port: 6123</p><p>jobmanager.bind-host: 0.0.0.0</p><p>jobmanager.memory.process.size: 1600m</p><p>taskmanager.bind-host: 0.0.0.0</p><p>taskmanager.host: hadoop01</p><p>taskmanager.memory.process.size: 1728m</p><p>taskmanager.numberOfTaskSlots: 1</p><p>parallelism.default: 1</p><p>jobmanager.execution.failover-strategy: region</p><p>rest.address: hadoop01</p><p>rest.bind-address: 0.0.0.0</p></blockquote><p>执行如下命令打开文件masters：</p><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stata"><span class="hljs-keyword">cd</span> /usr/<span class="hljs-keyword">local</span>/flink/<span class="hljs-keyword">conf</span><br>vim masters<br></code></pre></td></tr></table></figure><p>清空masters文件的原有内容，增加如下一行配置：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">hadoop01</span>:<span class="hljs-number">8081</span><br></code></pre></td></tr></table></figure><p>执行如下命令打开文件workers：</p><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stata"><span class="hljs-keyword">cd</span> /usr/<span class="hljs-keyword">local</span>/flink/<span class="hljs-keyword">conf</span><br>vim workers<br></code></pre></td></tr></table></figure><p>清空workers文件的原有内容，增加如下3行配置：</p><figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs smali">hadoop01<br>hadoop02<br>hadoop03<br></code></pre></td></tr></table></figure><h4 id="5-2-分发其他机子"><a href="#5-2-分发其他机子" class="headerlink" title="5.2 分发其他机子"></a>5.2 分发其他机子</h4><p>把hadoop01节点的安装文件发送到hadoop02和hadoop03节点</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span>  /usr/local/<br>tar  -zcf  ~/flink.master.tar.gz  ./flink<br><span class="hljs-built_in">cd</span>  ~<br>scp  ./flink.master.tar.gz  hadoop02:/home/hadoop<br>scp  ./flink.master.tar.gz  hadoop03:/home/hadoop<br><br></code></pre></td></tr></table></figure><p>在hadoop02和hadoop03节点上分别执行下面同样的操作：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> <span class="hljs-built_in">rm</span> -rf /usr/local/flink/<br><span class="hljs-built_in">sudo</span> tar -zxf ~/flink.master.tar.gz -C /usr/local<br><span class="hljs-built_in">sudo</span> <span class="hljs-built_in">chown</span> -R hadoop /usr/local/flink<br></code></pre></td></tr></table></figure><h4 id="5-3-修改hadoop02和hadoop03节点上的配置文件"><a href="#5-3-修改hadoop02和hadoop03节点上的配置文件" class="headerlink" title="5.3 修改hadoop02和hadoop03节点上的配置文件"></a>5.3 修改hadoop02和hadoop03节点上的配置文件</h4><p>在hadoop02节点上，把conf&#x2F;flink-conf.yaml中的taskmanager.host修改为如下内容：</p><p>vim &#x2F;usr&#x2F;local&#x2F;flink&#x2F;conf&#x2F;flink-conf.yaml</p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs avrasm"><span class="hljs-symbol">taskmanager.host:</span> hadoop02<br></code></pre></td></tr></table></figure><p>在hadoop03节点上，把conf&#x2F;flink-conf.yaml中的taskmanager.host修改为如下内容：</p><p>vim &#x2F;usr&#x2F;local&#x2F;flink&#x2F;conf&#x2F;flink-conf.yaml</p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs avrasm"><span class="hljs-symbol">taskmanager.host:</span> hadoop03<br></code></pre></td></tr></table></figure><p>在hadoop01节点上执行如下命令启动Flink集群：</p><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs stata"><span class="hljs-keyword">cd</span> /usr/<span class="hljs-keyword">local</span>/flink/<br>./bin/start-<span class="hljs-keyword">cluster</span>.<span class="hljs-keyword">sh</span><br></code></pre></td></tr></table></figure><p>启动以后，在hadoop01节点上执行jps命令，可以看到如下信息：</p><p>jps</p><p>7265 Jps</p><p>5829 StandaloneSessionClusterEntrypoint</p><p>6153 TaskManagerRunner</p><p>在hadoop02和hadoop03节点上分别执行jps命令，可以看到如下信息：</p><p>jps</p><p>4757 TaskManagerRunner</p><p>5639 Jps</p><p>如果能够看到上述信息，说明集群启动成功。</p><h4 id="5-4-查看Flink集群信息"><a href="#5-4-查看Flink集群信息" class="headerlink" title="5.4 查看Flink集群信息"></a>5.4 查看Flink集群信息</h4><p>启动成功以后，可以在hadoop01节点上打开浏览器，访问<a href="http://hadoop01:8081，就可以通过浏览器查看Flink集群信息">http://hadoop01:8081，就可以通过浏览器查看Flink集群信息</a></p><h3 id="2-WordCount案例"><a href="#2-WordCount案例" class="headerlink" title="2.WordCount案例"></a>2.WordCount案例</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /usr/local/flink<br>./bin/flink run examples/streaming/WordCount.jar<br></code></pre></td></tr></table></figure><p>查看结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">tail</span> <span class="hljs-built_in">log</span>/flink-*-taskexecutor-*.out<br></code></pre></td></tr></table></figure><h2 id="使用Idea开发Flink应用程序"><a href="#使用Idea开发Flink应用程序" class="headerlink" title="&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;使用Idea开发Flink应用程序"></a>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;使用Idea开发Flink应用程序</h2><h3 id="1-Idea安装"><a href="#1-Idea安装" class="headerlink" title="1.Idea安装"></a>1.Idea安装</h3><blockquote><p>下载地址：<a target="_blank" rel="noopener" href="https://www.jetbrains.com/idea/download/">https://www.jetbrains.com/idea/download/</a></p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> ~<br><span class="hljs-built_in">sudo</span> tar -zxvf ideaIC-2025.2.1.tar.gz -C /usr/local <br><span class="hljs-built_in">cd</span> /usr/local<br><span class="hljs-built_in">sudo</span> <span class="hljs-built_in">mv</span> ./idea-IC-252.25557.131/ ./idea<br><span class="hljs-built_in">sudo</span> <span class="hljs-built_in">chown</span> -R hadoop ./idea   <br></code></pre></td></tr></table></figure><h3 id="2-启动"><a href="#2-启动" class="headerlink" title="2.启动"></a>2.启动</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /usr/local/idea<br>./bin/idea.sh<br></code></pre></td></tr></table></figure><h3 id="3-idea的操作"><a href="#3-idea的操作" class="headerlink" title="3.idea的操作"></a>3.idea的操作</h3><p><img src="/./assets/image-20250930143208254.png" srcset="/img/loading.gif" lazyload alt="image-20250930143208254"></p><p><img src="/./assets/image-20250930143216645.png" srcset="/img/loading.gif" lazyload alt="image-20250930143216645"></p><h3 id="程序大致过程讲解（面试风格）"><a href="#程序大致过程讲解（面试风格）" class="headerlink" title="程序大致过程讲解（面试风格）"></a>程序大致过程讲解（面试风格）</h3><p>面试官您好，这个程序是一个使用 Apache Flink 框架实现的<strong>批处理词频统计</strong> 作业。它的核心目标是读取一段文本，并计算出其中每个单词出现的总次数。</p><p>它的执行过程可以概括为以下几个核心步骤：</p><h4 id="1-程序入口与环境准备-WordCount-main"><a href="#1-程序入口与环境准备-WordCount-main" class="headerlink" title="1. 程序入口与环境准备 (WordCount.main)"></a>1. 程序入口与环境准备 (<code>WordCount.main</code>)</h4><p>这是整个作业的指挥官，负责搭建舞台和调度整个流程。</p><figure class="highlight mel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs mel"><span class="hljs-comment">// 伪代码：步骤1 - 初始化</span><br>function main() &#123;<br>    <span class="hljs-comment">// 1. 获取Flink的流执行环境（这是所有计算的基石）</span><br>    <span class="hljs-keyword">env</span> = StreamExecutionEnvironment.getExecutionEnvironment();<br>    <br>    <span class="hljs-comment">// 2. **关键设置**：明确指定为“批处理”模式。</span><br>    <span class="hljs-comment">//    这意味着程序会认为输入数据是有边界的，会在所有数据到达后一次性给出最终结果。</span><br>    <span class="hljs-keyword">env</span>.setRuntimeMode(RUNTIME_MODE_BATCH);<br>    <br>    <span class="hljs-comment">// 3. 调用后续步骤，构建数据处理流程（也称为DAG图）</span><br>    buildDataFlow(<span class="hljs-keyword">env</span>);<br>    <br>    <span class="hljs-comment">// 4. 最终下令：启动并执行这个已经构建好的计算任务。</span><br>    <span class="hljs-keyword">env</span>.execute();<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="2-数据源接入-WordCountData-getDefaultTextLineDataStream"><a href="#2-数据源接入-WordCountData-getDefaultTextLineDataStream" class="headerlink" title="2. 数据源接入 (WordCountData.getDefaultTextLineDataStream)"></a>2. 数据源接入 (<code>WordCountData.getDefaultTextLineDataStream</code>)</h4><p>这个步骤负责为程序提供原材料，即待处理的文本数据。</p><figure class="highlight wren"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs wren"><span class="hljs-comment">// 伪代码：步骤2 - 准备数据</span><br><span class="hljs-variable">function</span> <span class="hljs-title function_">getDefaultTextLineDataStream</span>(<span class="hljs-params">env</span>) &#123;<br>    <span class="hljs-comment">// 从一个预定义的、硬编码的字符串数组（莎士比亚的独白）中创建数据流。</span><br>    <span class="hljs-comment">// 在批处理模式下，Flink会一次性读取这个数组的所有元素。</span><br>    <span class="hljs-variable">dataStream</span> <span class="hljs-operator">=</span> <span class="hljs-variable">env</span>.<span class="hljs-property">fromElements</span>(<span class="hljs-string">&quot;To be, or not to be...&quot;</span>, <span class="hljs-string">&quot;Whether &#x27;tis nobler...&quot;</span>, <span class="hljs-operator">...</span>);<br>    <span class="hljs-keyword">return</span> <span class="hljs-variable">dataStream</span>;<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="3-核心转换逻辑-WordCountTokenizer-flatMap-及后续操作"><a href="#3-核心转换逻辑-WordCountTokenizer-flatMap-及后续操作" class="headerlink" title="3. 核心转换逻辑 (WordCountTokenizer.flatMap 及后续操作)"></a>3. 核心转换逻辑 (<code>WordCountTokenizer.flatMap</code> 及后续操作)</h4><p>这是程序的“大脑”，负责将原始的文本行转换成可计算的（单词，计数）对。这是一个典型的三步流水线。</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-regexp">//</span> 伪代码：步骤<span class="hljs-number">3</span> - 数据处理流水线<br><span class="hljs-regexp">//</span> 假设我们有一个数据流，元素为：[<span class="hljs-string">&quot;To be or not to be&quot;</span>, <span class="hljs-string">&quot;that is the question&quot;</span>]<br><br><span class="hljs-regexp">//</span> <span class="hljs-number">3.1</span> 分词与标准化 (FlatMap)<br><span class="hljs-keyword">function</span> flatMap(line) &#123;<br>    <span class="hljs-regexp">//</span> 将一行文本转换为小写，并用非字母数字字符（如空格、逗号）切分成单词。<br>    <span class="hljs-regexp">//</span> 例如，输入 <span class="hljs-string">&quot;To be, or not to be&quot;</span> -&gt; 得到 tokens: [<span class="hljs-string">&quot;to&quot;</span>, <span class="hljs-string">&quot;be&quot;</span>, <span class="hljs-string">&quot;or&quot;</span>, <span class="hljs-string">&quot;not&quot;</span>, <span class="hljs-string">&quot;to&quot;</span>, <span class="hljs-string">&quot;be&quot;</span>]<br>    tokens = line.toLowerCase().split(<span class="hljs-string">&quot;\\W+&quot;</span>);<br>    <br>    <span class="hljs-regexp">//</span> 为每一个有效的单词，输出一个 (单词, <span class="hljs-number">1</span>) 的键值对。<br>    <span class="hljs-regexp">//</span> 例如，对于 <span class="hljs-string">&quot;to&quot;</span>，输出 (<span class="hljs-string">&quot;to&quot;</span>, <span class="hljs-number">1</span>); 对于 <span class="hljs-string">&quot;be&quot;</span>，输出 (<span class="hljs-string">&quot;be&quot;</span>, <span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">for</span> each token <span class="hljs-keyword">in</span> tokens &#123;<br>        <span class="hljs-keyword">if</span> (token is not empty) &#123;<br>            emit (token, <span class="hljs-number">1</span>);<br>        &#125;<br>    &#125;<br>&#125;<br><span class="hljs-regexp">//</span> 经过此步骤，数据流变成了：(<span class="hljs-string">&quot;to&quot;</span>,<span class="hljs-number">1</span>), (<span class="hljs-string">&quot;be&quot;</span>,<span class="hljs-number">1</span>), (<span class="hljs-string">&quot;or&quot;</span>,<span class="hljs-number">1</span>), (<span class="hljs-string">&quot;not&quot;</span>,<span class="hljs-number">1</span>), (<span class="hljs-string">&quot;to&quot;</span>,<span class="hljs-number">1</span>), (<span class="hljs-string">&quot;be&quot;</span>,<span class="hljs-number">1</span>), (<span class="hljs-string">&quot;that&quot;</span>,<span class="hljs-number">1</span>), (<span class="hljs-string">&quot;is&quot;</span>,<span class="hljs-number">1</span>), (<span class="hljs-string">&quot;the&quot;</span>,<span class="hljs-number">1</span>), (<span class="hljs-string">&quot;question&quot;</span>,<span class="hljs-number">1</span>) ...<br><br><span class="hljs-regexp">//</span> <span class="hljs-number">3.2</span> 分组 (KeyBy)<br><span class="hljs-regexp">//</span> 按照元组的第一个字段（即单词）进行分组。<br><span class="hljs-regexp">//</span> 这相当于在Flink内部创建了不同的“桶”，所有相同的单词都会被分配到同一个桶里。<br><span class="hljs-regexp">//</span> 分组后逻辑上形成： [<span class="hljs-string">&quot;to&quot;</span>桶: (<span class="hljs-string">&quot;to&quot;</span>,<span class="hljs-number">1</span>), (<span class="hljs-string">&quot;to&quot;</span>,<span class="hljs-number">1</span>)], [<span class="hljs-string">&quot;be&quot;</span>桶: (<span class="hljs-string">&quot;be&quot;</span>,<span class="hljs-number">1</span>), (<span class="hljs-string">&quot;be&quot;</span>,<span class="hljs-number">1</span>)], [<span class="hljs-string">&quot;or&quot;</span>桶: (<span class="hljs-string">&quot;or&quot;</span>,<span class="hljs-number">1</span>)] ...<br><br><span class="hljs-regexp">//</span> <span class="hljs-number">3.3</span> 聚合求和 (Sum)<br><span class="hljs-regexp">//</span> 在每个“单词桶”内，对元组的第二个字段（即计数<span class="hljs-number">1</span>）进行累加求和。<br><span class="hljs-regexp">//</span> 例如，在 <span class="hljs-string">&quot;to&quot;</span> 桶里：<span class="hljs-number">1</span> + <span class="hljs-number">1</span> = <span class="hljs-number">2</span> -&gt; (<span class="hljs-string">&quot;to&quot;</span>, <span class="hljs-number">2</span>)<br><span class="hljs-regexp">//</span>       在 <span class="hljs-string">&quot;be&quot;</span> 桶里：<span class="hljs-number">1</span> + <span class="hljs-number">1</span> = <span class="hljs-number">2</span> -&gt; (<span class="hljs-string">&quot;be&quot;</span>, <span class="hljs-number">2</span>)<br><span class="hljs-regexp">//</span> 最终结果流：(<span class="hljs-string">&quot;to&quot;</span>,<span class="hljs-number">2</span>), (<span class="hljs-string">&quot;be&quot;</span>,<span class="hljs-number">2</span>), (<span class="hljs-string">&quot;or&quot;</span>,<span class="hljs-number">1</span>), (<span class="hljs-string">&quot;not&quot;</span>,<span class="hljs-number">1</span>), (<span class="hljs-string">&quot;that&quot;</span>,<span class="hljs-number">1</span>), (<span class="hljs-string">&quot;is&quot;</span>,<span class="hljs-number">1</span>), (<span class="hljs-string">&quot;the&quot;</span>,<span class="hljs-number">1</span>), (<span class="hljs-string">&quot;question&quot;</span>,<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><h4 id="总结与输出"><a href="#总结与输出" class="headerlink" title="总结与输出"></a>总结与输出</h4><p>最终，这个聚合后的结果流 <code>counts</code> 会被 <code>print()</code> sink 输出到标准输出（比如您的IDE控制台）。由于我们设置了批处理模式，所以会在所有数据处理完毕后，<strong>一次性打印出最终的、完整的词频统计结果</strong>，看起来会像一个列表。</p><h4 id="3-1-WordCountData-java"><a href="#3-1-WordCountData-java" class="headerlink" title="3.1 WordCountData.java"></a>3.1 WordCountData.java</h4><figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs swift"><span class="hljs-keyword">package</span> cn.edu.pz;<br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;<br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;<br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">WordCountData</span> &#123;<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-type">String</span>[] <span class="hljs-type">WORDS</span><span class="hljs-operator">=</span>new <span class="hljs-type">String</span>[]&#123;<span class="hljs-string">&quot;To be, or not to be,--that is the question:--&quot;</span>, <span class="hljs-string">&quot;Whether <span class="hljs-subst">\&#x27;</span>tis nobler in the mind to suffer&quot;</span>, <span class="hljs-string">&quot;The slings and arrows of outrageous fortune&quot;</span>, <span class="hljs-string">&quot;Or to take arms against a sea of troubles,&quot;</span>, <span class="hljs-string">&quot;And by opposing end them?--To die,--to sleep,--&quot;</span>, <span class="hljs-string">&quot;No more; and by a sleep to say we end&quot;</span>, <span class="hljs-string">&quot;The heartache, and the thousand natural shocks&quot;</span>, <span class="hljs-string">&quot;That flesh is heir to,--<span class="hljs-subst">\&#x27;</span>tis a consummation&quot;</span>, <span class="hljs-string">&quot;Devoutly to be wish<span class="hljs-subst">\&#x27;</span>d. To die,--to sleep;--&quot;</span>, <span class="hljs-string">&quot;To sleep! perchance to dream:--ay, there<span class="hljs-subst">\&#x27;</span>s the rub;&quot;</span>, <span class="hljs-string">&quot;For in that sleep of death what dreams may come,&quot;</span>, <span class="hljs-string">&quot;When we have shuffled off this mortal coil,&quot;</span>, <span class="hljs-string">&quot;Must give us pause: there<span class="hljs-subst">\&#x27;</span>s the respect&quot;</span>, <span class="hljs-string">&quot;That makes calamity of so long life;&quot;</span>, <span class="hljs-string">&quot;For who would bear the whips and scorns of time,&quot;</span>, <span class="hljs-string">&quot;The oppressor<span class="hljs-subst">\&#x27;</span>s wrong, the proud man<span class="hljs-subst">\&#x27;</span>s contumely,&quot;</span>, <span class="hljs-string">&quot;The pangs of despis<span class="hljs-subst">\&#x27;</span>d love, the law<span class="hljs-subst">\&#x27;</span>s delay,&quot;</span>, <span class="hljs-string">&quot;The insolence of office, and the spurns&quot;</span>, <span class="hljs-string">&quot;That patient merit of the unworthy takes,&quot;</span>, <span class="hljs-string">&quot;When he himself might his quietus make&quot;</span>, <span class="hljs-string">&quot;With a bare bodkin? who would these fardels bear,&quot;</span>, <span class="hljs-string">&quot;To grunt and sweat under a weary life,&quot;</span>, <span class="hljs-string">&quot;But that the dread of something after death,--&quot;</span>, <span class="hljs-string">&quot;The undiscover<span class="hljs-subst">\&#x27;</span>d country, from whose bourn&quot;</span>, <span class="hljs-string">&quot;No traveller returns,--puzzles the will,&quot;</span>, <span class="hljs-string">&quot;And makes us rather bear those ills we have&quot;</span>, <span class="hljs-string">&quot;Than fly to others that we know not of?&quot;</span>, <span class="hljs-string">&quot;Thus conscience does make cowards of us all;&quot;</span>, <span class="hljs-string">&quot;And thus the native hue of resolution&quot;</span>, <span class="hljs-string">&quot;Is sicklied o<span class="hljs-subst">\&#x27;</span>er with the pale cast of thought;&quot;</span>, <span class="hljs-string">&quot;And enterprises of great pith and moment,&quot;</span>, <span class="hljs-string">&quot;With this regard, their currents turn awry,&quot;</span>, <span class="hljs-string">&quot;And lose the name of action.--Soft you now!&quot;</span>, <span class="hljs-string">&quot;The fair Ophelia!--Nymph, in thy orisons&quot;</span>, <span class="hljs-string">&quot;Be all my sins remember<span class="hljs-subst">\&#x27;</span>d.&quot;</span>&#125;;<br>    <span class="hljs-keyword">public</span> <span class="hljs-type">WordCountData</span>() &#123;<br>    &#125;<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-type">DataStream</span>&lt;<span class="hljs-type">String</span>&gt; getDefaultTextLineDataStream(<span class="hljs-type">StreamExecutionEnvironment</span> env)&#123;<br>        <span class="hljs-keyword">return</span> env.fromElements(<span class="hljs-type">WORDS</span>);<br>    &#125;<br>    &#125;<br></code></pre></td></tr></table></figure><h4 id="3-2-WordCountTokenizer-java"><a href="#3-2-WordCountTokenizer-java" class="headerlink" title="3.2 WordCountTokenizer.java"></a>3.2 WordCountTokenizer.java</h4><p>用于切分句子，其内容如下：</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs arduino">package cn.edu.pz;<br><span class="hljs-keyword">import</span> org.apache.flink.api.common.functions.FlatMapFunction;<br><span class="hljs-keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;<br><span class="hljs-keyword">import</span> org.apache.flink.util.Collector;<br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">WordCountTokenizer</span> implements FlatMapFunction&lt;<span class="hljs-type">String</span>, Tuple2&lt;<span class="hljs-type">String</span>,Integer&gt;&gt;&#123;<br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">WordCountTokenizer</span><span class="hljs-params">()</span></span>&#123;&#125;<br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-type">void</span> <span class="hljs-title">flatMap</span><span class="hljs-params">(<span class="hljs-type">String</span> value, Collector&lt;Tuple2&lt;<span class="hljs-type">String</span>, Integer&gt;&gt; out)</span> throws Exception </span>&#123;<br>        <span class="hljs-type">String</span>[] tokens = value.<span class="hljs-built_in">toLowerCase</span>().<span class="hljs-built_in">split</span>(<span class="hljs-string">&quot;\\W+&quot;</span>);<br>        <span class="hljs-type">int</span> len = tokens.length;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i&lt;len;i++)&#123;<br>            <span class="hljs-type">String</span> tmp = tokens[i];<br>            <span class="hljs-keyword">if</span>(tmp.<span class="hljs-built_in">length</span>()&gt;<span class="hljs-number">0</span>)&#123;<br>                out.<span class="hljs-built_in">collect</span>(<span class="hljs-keyword">new</span> <span class="hljs-built_in">Tuple2</span>&lt;<span class="hljs-type">String</span>, Integer&gt;(tmp,Integer.<span class="hljs-built_in">valueOf</span>(<span class="hljs-number">1</span>)));<br>            &#125;<br>        &#125;<br>    &#125;<br>    &#125;<br></code></pre></td></tr></table></figure><h4 id="3-3-WordCount-java"><a href="#3-3-WordCount-java" class="headerlink" title="3.3 WordCount.java"></a>3.3 WordCount.java</h4><p>提供主函数，其内容如下：</p><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs arduino">package cn.edu.pz;<br><span class="hljs-keyword">import</span> org.apache.flink.api.common.RuntimeExecutionMode;<br><span class="hljs-keyword">import</span> org.apache.flink.api.java.tuple.Tuple2;<br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.datastream.DataStream;<br><span class="hljs-keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;<br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">WordCount</span> &#123;<br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">WordCount</span><span class="hljs-params">()</span></span>&#123;&#125;<br>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-type">static</span> <span class="hljs-type">void</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-type">String</span>[] args)</span> throws Exception </span>&#123;<br>        StreamExecutionEnvironment env = StreamExecutionEnvironment.<span class="hljs-built_in">getExecutionEnvironment</span>();<br>        env.<span class="hljs-built_in">setRuntimeMode</span>(RuntimeExecutionMode.BATCH);<br>        Object text;<br>        text = WordCountData.<span class="hljs-built_in">getDefaultTextLineDataStream</span>(env);<br>        DataStream&lt;Tuple2&lt;<span class="hljs-type">String</span>, Integer&gt;&gt; counts = ((DataStream&lt;<span class="hljs-type">String</span>&gt;)text).<span class="hljs-built_in">flatMap</span>(<span class="hljs-keyword">new</span> <span class="hljs-built_in">WordCountTokenizer</span>())<br>                .<span class="hljs-built_in">keyBy</span>(<span class="hljs-number">0</span>)<br>                .<span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span>);<br>        counts.<span class="hljs-built_in">print</span>();<br>        env.<span class="hljs-built_in">execute</span>();<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="3-4-pom-xml"><a href="#3-4-pom-xml" class="headerlink" title="3.4 pom.xml"></a>3.4 pom.xml</h4><p>修改项目下的pom.xml文件内容，增加输入如下内容：</p><figure class="highlight dust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs dust"><span class="language-xml"><span class="hljs-tag">&lt;<span class="hljs-name">repositories</span>&gt;</span></span><br><span class="language-xml"><span class="hljs-tag">&lt;<span class="hljs-name">repository</span>&gt;</span></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">id</span>&gt;</span>alimaven<span class="hljs-tag">&lt;/<span class="hljs-name">id</span>&gt;</span></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>aliyun maven<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">url</span>&gt;</span>https://maven.aliyun.com/nexus/content/groups/public/<span class="hljs-tag">&lt;/<span class="hljs-name">url</span>&gt;</span></span><br><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">repository</span>&gt;</span></span><br><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">repositories</span>&gt;</span></span><br><span class="language-xml"></span><br><span class="language-xml"><span class="hljs-tag">&lt;<span class="hljs-name">properties</span>&gt;</span></span><br><span class="language-xml"><span class="hljs-tag">&lt;<span class="hljs-name">flink.version</span>&gt;</span>1.17.0<span class="hljs-tag">&lt;/<span class="hljs-name">flink.version</span>&gt;</span></span><br><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">properties</span>&gt;</span></span><br><span class="language-xml"></span><br><span class="language-xml"><span class="hljs-tag">&lt;<span class="hljs-name">dependencies</span>&gt;</span></span><br><span class="language-xml"><span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.flink<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>flink-streaming-java<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>$</span><span class="hljs-template-variable">&#123;flink.version&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span></span><br><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml"><span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.flink<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>flink-clients<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>$</span><span class="hljs-template-variable">&#123;flink.version&#125;</span><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span></span><br><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span></span><br><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">dependencies</span>&gt;</span></span><br><span class="language-xml"></span><br><span class="language-xml"><span class="hljs-tag">&lt;<span class="hljs-name">build</span>&gt;</span></span><br><span class="language-xml"><span class="hljs-tag">&lt;<span class="hljs-name">plugins</span>&gt;</span></span><br><span class="language-xml">  <span class="hljs-tag">&lt;<span class="hljs-name">plugin</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>3.0.0<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">descriptorRefs</span>&gt;</span></span><br><span class="language-xml">        <span class="hljs-tag">&lt;<span class="hljs-name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="hljs-tag">&lt;/<span class="hljs-name">descriptorRef</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;/<span class="hljs-name">descriptorRefs</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;<span class="hljs-name">executions</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;<span class="hljs-name">execution</span>&gt;</span></span><br><span class="language-xml">        <span class="hljs-tag">&lt;<span class="hljs-name">id</span>&gt;</span>make-assembly<span class="hljs-tag">&lt;/<span class="hljs-name">id</span>&gt;</span></span><br><span class="language-xml">        <span class="hljs-tag">&lt;<span class="hljs-name">phase</span>&gt;</span>package<span class="hljs-tag">&lt;/<span class="hljs-name">phase</span>&gt;</span></span><br><span class="language-xml">        <span class="hljs-tag">&lt;<span class="hljs-name">goals</span>&gt;</span></span><br><span class="language-xml">          <span class="hljs-tag">&lt;<span class="hljs-name">goal</span>&gt;</span>single<span class="hljs-tag">&lt;/<span class="hljs-name">goal</span>&gt;</span></span><br><span class="language-xml">        <span class="hljs-tag">&lt;/<span class="hljs-name">goals</span>&gt;</span></span><br><span class="language-xml">      <span class="hljs-tag">&lt;/<span class="hljs-name">execution</span>&gt;</span></span><br><span class="language-xml">    <span class="hljs-tag">&lt;/<span class="hljs-name">executions</span>&gt;</span></span><br><span class="language-xml">  <span class="hljs-tag">&lt;/<span class="hljs-name">plugin</span>&gt;</span></span><br><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">plugins</span>&gt;</span></span><br><span class="language-xml"><span class="hljs-tag">&lt;/<span class="hljs-name">build</span>&gt;</span></span><br></code></pre></td></tr></table></figure><h4 id="3-5-运行效果"><a href="#3-5-运行效果" class="headerlink" title="3.5 运行效果"></a>3.5 运行效果</h4><p><img src="/./assets/image-20250930143547035.png" srcset="/img/loading.gif" lazyload alt="image-20250930143547035"></p><h3 id="4-命令提交运行程序"><a href="#4-命令提交运行程序" class="headerlink" title="4.命令提交运行程序"></a>4.命令提交运行程序</h3><p>可以执行如下命令提交运行程序：</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli"><span class="hljs-keyword">cd</span> <span class="hljs-string">/usr/local/flink</span><br><span class="hljs-string">./bin/flink</span> run <span class="hljs-params">--class</span> cn.edu.pz.WordCount ~<span class="hljs-string">/WordCount-1.0-SNAPSHOT.jar</span><br></code></pre></td></tr></table></figure><h3 id="5-向Flink提交运行程序"><a href="#5-向Flink提交运行程序" class="headerlink" title="5.向Flink提交运行程序"></a>5.向Flink提交运行程序</h3><blockquote><p>在IDEA中程序调试运行成功以后，可以对程序进行打包，以便部署到Flink平台上。具体方法是：在项目开发界面的右边，点击“Maven”按钮，然后在弹出的界面中（如图4-7所示），双击“package”，就可以完成对应用程序的打包。</p><p>打包成功以后，可以在项目开发界面左侧的目录树中，在target子目录下找到两个文件“wordcount-1.0.jar”和“wordcount-1.0-jar-with-dependencies.jar”，其中，wordcount-1.0.jar在运行时需要由运行环境提供相关依赖JAR包，而wordcount-1.0-jar-with-dependencies.jar则集成了所有相关的依赖JAR包，不需要运行环境提供相关依赖JAR包支持。</p></blockquote><p><img src="/./assets/image-20250930143706097.png" srcset="/img/loading.gif" lazyload alt="image-20250930143706097"></p><p>浏览器打开hadoop01:8081</p><p><img src="/./assets/image-20250930143805015.png" srcset="/img/loading.gif" lazyload alt="image-20250930143805015"></p><p><img src="/./assets/image-20250930143811168.png" srcset="/img/loading.gif" lazyload alt="image-20250930143811168"></p><p><img src="/./assets/image-20250930143817053.png" srcset="/img/loading.gif" lazyload alt="image-20250930143817053"></p><p><img src="/./assets/image-20250930143833109.png" srcset="/img/loading.gif" lazyload alt="image-20250930143833109"></p><p><img src="/./assets/image-20250930143843798.png" srcset="/img/loading.gif" lazyload alt="image-20250930143843798"></p><h2 id="3种运行模式"><a href="#3种运行模式" class="headerlink" title="&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;3种运行模式"></a>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;3种运行模式</h2><h3 id="1-会话模式"><a href="#1-会话模式" class="headerlink" title="1.会话模式"></a>1.会话模式</h3><blockquote><p>先启动一个集群，保持一个会话，在这个会话中通过客户端提交作业。集群启动时所有资源就已经确定，所以，所有提交的作业会竞争集群中的资源.</p></blockquote><p><img src="/./assets/image-20250930160032008.png" srcset="/img/loading.gif" lazyload alt="image-20250930160032008"></p><h3 id="2-单作业模式"><a href="#2-单作业模式" class="headerlink" title="2.单作业模式"></a>2.单作业模式</h3><blockquote><p>单作业模式是严格的一对一，集群只为这个作业而生。由客户端运行应用程序，然后启动集群，作业被提交给JobManager，进而分发给TaskManager执行。</p><p>作业完成后，集群就会关闭，所有资源会被释放。</p><p>每个作业都由它自己的JobManager管理，占用独享的资源，即使发生故障导致它的TaskManager 宕机，也不会影响其他作业。</p></blockquote><p><img src="/./assets/image-20250930160521357.png" srcset="/img/loading.gif" lazyload alt="image-20250930160521357"></p><h3 id="3-应用模式"><a href="#3-应用模式" class="headerlink" title="3.应用模式"></a>3.应用模式</h3><blockquote><p>在会话模式和单作业模式中，应用代码在客户端上执行，然后由客户端提交给JobManager，客户端需要占用大量网络带宽用于下载依赖和把二进制数据发送给JobManager；并且提交作业用的是同一个客户端，就会加重客户端所在节点的资源消耗。</p><p>解决办法是直接把应用提交到JobManger上运行。为每一个提交的应用单独启动一个JobManager，也就是创建一个集群。这个JobManager只为执行这一个应用而存在，执行结束之后JobManager也就关闭了，这就是所谓的“应用模式”</p></blockquote><p><img src="/./assets/image-20250930160759311.png" srcset="/img/loading.gif" lazyload alt="image-20250930160759311"></p><h3 id="Standalone模式部署集群"><a href="#Standalone模式部署集群" class="headerlink" title="Standalone模式部署集群"></a>Standalone模式部署集群</h3><p>会话模式 ✓</p><p>应用模式 ✓</p><p>单作业模式 ×</p><blockquote><p>当采用Standalone模式部署集群时，Flink集群是独立运行的，不依赖任何外部的资源管理器。</p><p>不过，需要注意的是，在Standalone模式下，如果资源不足，或者出现故障，集群没有自动扩展或重新分配资源的保证，必须手动处理，所以，这种模式一般只用在开发测试或作业非常少的场景下。</p><p>当采用Standalone模式部署集群时，可以支持两种运行模式，即会话模式和应用模式，不支持单作业模式。</p><p>会话模式是采用Standalone模式部署集群时默认采用的运行模式，之前在4.5.5节中运行wordcount-1.0.jar，就是采用了会话模式。因此，下面只介绍Standalone部署模式下采用应用模式的场景。</p></blockquote><p>在应用模式下，不会提前创建集群，所以，不能调用start-cluster.sh脚本（如果此前已经启动集群，需要使用stop-cluster.sh停止集群）。</p><p>要使用bin目录下的standalone-job.sh来创建一个JobManager。</p><p>具体步骤如下（下面的操作都是在hadoop01节点中执行）：</p><p>（1）把应用程序JAR包wordcount-1.0.jar放到“&#x2F;usr&#x2F;local&#x2F;flink&#x2F;lib”目录下。</p><p>（2）使用bin目录下的standalone-job.sh启动JobManager，命令如下：</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli"><span class="hljs-keyword">cd</span> <span class="hljs-string">/usr/local/flink</span><br><span class="hljs-string">./bin/standalone-job.sh</span> start <span class="hljs-params">--job-classname</span> cn.edu.pz.WordCount<br></code></pre></td></tr></table></figure><blockquote><p>执行jps命令，可以看到一个名称为“StandaloneApplicationClusterEntryPoint”的进程。</p><p>在浏览器中输入“<a href="http://hadoop01:8081”，在Web页面中可以看到JobManager已经启动。">http://hadoop01:8081”，在Web页面中可以看到JobManager已经启动。</a></p><p>这时，程序不会执行，因为，还没有创建TaskManager，可用的Task Slot是0，无法执行作业。</p></blockquote><p>（3）使用bin目录下的taskmanager.sh启动TaskManager，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /usr/local/flink<br>./bin/taskmanager.sh start<br></code></pre></td></tr></table></figure><blockquote><p>这时立即执行jps命令，可以看到“StandaloneApplicationClusterEntryPoint”和“TaskManagerRunner”两个进程。</p><p>由于TaskManager已经创建，可以分配Task Slot给程序运行，所以，wordcount-1.0.jar程序就会开始执行，执行结束后，JobManager自动关闭，所以，经过几秒钟以后再去执行jps命令，“StandaloneApplicationClusterEntryPoint”进程已经消失了，只剩余“TaskManagerRunner”进程，这时再去访问“<a href="http://hadoop01:8081”，就会无法访问。">http://hadoop01:8081”，就会无法访问。</a></p></blockquote><p>需要指出的是，由于wordcount-1.0.jar程序中使用了语句env.setRuntimeMode(RuntimeExecutionMode.BATCH)，程序被设置为批处理模式，</p><p>所以，批处理结束以后，程序自动结束，JobManager自动关闭。但是，如果以后运行一个流处理程序，则需要执行如下命令手动关闭JobManager:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /usr/local/flink<br>./bin/standalone-job.sh stop<br></code></pre></td></tr></table></figure><p>（4）手动执行如下命令结束TaskManager：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /usr/local/flink<br>./bin/taskmanager.sh stop<br></code></pre></td></tr></table></figure><blockquote><p>上面命令执行以后，再执行jps命令以后，“TaskManagerRunner”进程也消失了。</p></blockquote><h3 id="YARN模式集群配置（重点）"><a href="#YARN模式集群配置（重点）" class="headerlink" title="YARN模式集群配置（重点）"></a>YARN模式集群配置（重点）</h3><p>在之前已经搭建好的Standalone模式的集群之上进行修改，得到YARN模式集群。</p><p>在hadoop01节点上修改环境变量配置文件.bashrc：</p><p>在.bashrc文件中增加如下配置信息（如果文件中已经存在这些配置信息，则不需要重复操作）：</p><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs elixir">export <span class="hljs-title class_">HADOOP_HOME</span>=/usr/local/hadoop<br>export <span class="hljs-title class_">PATH</span>=<span class="hljs-variable">$PATH</span><span class="hljs-symbol">:/usr/local/hadoop/bin</span><span class="hljs-symbol">:/usr/local/hadoop/sbin</span><br>export <span class="hljs-title class_">HADOOP_CLASSPATH</span>=`hadoop classpath`<br>export <span class="hljs-title class_">HADOOP_CONF_DIR</span>=<span class="hljs-variable">$HADOOP_HOME</span>/etc/hadoop<br></code></pre></td></tr></table></figure><p>source ~&#x2F;.bashrc</p><p>启动HDFS和YARN：</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">hadoop <span class="hljs-literal">start</span>-dfs.sh<br>hadoop <span class="hljs-literal">start</span>-yarn.sh<br></code></pre></td></tr></table></figure><p><a target="_blank" rel="noopener" href="http://hadoop01:8088/">http://hadoop01:8088</a></p><p><img src="/./assets/image-20250930163357201.png" srcset="/img/loading.gif" lazyload alt="image-20250930163357201"></p><h4 id="会话模式"><a href="#会话模式" class="headerlink" title="会话模式"></a>会话模式</h4><p>正常启动hadoop之后，</p><p>在Flink的安装目录下的bin目录下，提供了一个yarn-session.sh脚本文件，可以使用如下命令查看yarn-session.sh的用法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /usr/local/flink<br>./bin/yarn-session.sh --<span class="hljs-built_in">help</span><br></code></pre></td></tr></table></figure><p>看完了吧，然后现在是不使用任何参数启动yarn-session.sh：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /usr/local/flink<br>./bin/yarn-session.sh<br></code></pre></td></tr></table></figure><p>启动成功后的端口不是固定的，所以推荐这样来访问集群管理Web页面：hadoop01:8088&#x2F;cluster</p><p><img src="/./assets/image-20250930163828381.png" srcset="/img/loading.gif" lazyload alt="image-20250930163828381"></p><blockquote><p>想后台运行就加一个d参数，.&#x2F;bin&#x2F;yarn-session.sh -d。</p><p>启动结束后，使用JPS命令查看进程，可以看到一个名字为“YarnSessionClusterEntrypoint”的进程。</p></blockquote><p>启动结束后，屏幕上会返回如图4-28所示的信息，注意箭头指向的这行信息“yarn application -kill application_1691735595654_0004”（每次启动返回的这行信息都是不同的），后面需要使用这条命令来关闭会话模式。</p><p><img src="/./assets/image-20250930164643446.png" srcset="/img/loading.gif" lazyload alt="image-20250930164643446"></p><p>这时，可以执行如下命令运行程序：</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli"><span class="hljs-keyword">cd</span> <span class="hljs-string">/usr/local/flink</span><br><span class="hljs-string">./bin/flink</span> run <span class="hljs-params">--class</span> cn.edu.pz.WordCount ~<span class="hljs-string">/Downloads/wordcount-1.0.jar</span><br></code></pre></td></tr></table></figure><blockquote><p>可以看出，上面的命令中并没有设置参数“–jobmanager”，这是因为在YARN模式中，Flink可以自动找到JobManager。</p></blockquote><p>在“Path ID”下面多了一条记录，点击这条记录中的链接，在出现的页面中点击“Stdout”,就可以看到词频统计结果了。</p><img src="./assets/image-20250930165920332-1759222761232-1.png" srcset="/img/loading.gif" lazyload alt="image-20250930165920332" style="zoom:80%"><p>关闭会话模式则：yarn application -kill application_1691735595654_0004</p><h4 id="单作业模式"><a href="#单作业模式" class="headerlink" title="单作业模式"></a>单作业模式</h4><p>在采用单作业模式时，不需要事先启动Flink集群。</p><p>启动hadoop后执行如下命令执行程序：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /usr/local/flink<br>./bin/flink run -t yarn-per-job -c cn.edu.pz.WordCount ~/Downloads/wordcount-1.0.jar<br></code></pre></td></tr></table></figure><p>程序运行结束后，可以在浏览器中输入“<a href="http://hadoop01:8088”查看集群信息（如图4-30所示），从中可以看到，“Name”这个列的值是“Flink">http://hadoop01:8088”查看集群信息（如图4-30所示），从中可以看到，“Name”这个列的值是“Flink</a> per-job Cluster”。</p><img src="./assets/image-20250930170211403.png" srcset="/img/loading.gif" lazyload alt="image-20250930170211403" style="zoom:80%"><h4 id="应用模式"><a href="#应用模式" class="headerlink" title="应用模式"></a>应用模式</h4><p>启动HDFS和YARN之后，在hadoop01节点上执行如下命令执行程序：</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs applescript">cd /usr/<span class="hljs-keyword">local</span>/flink<br>./bin/flink <span class="hljs-built_in">run</span>-<span class="hljs-built_in">application</span> \<br>-t yarn-<span class="hljs-built_in">application</span> \<br>-c cn.edu.pz.WordCount \<br>~/wordcount<span class="hljs-number">-1.0</span>.jar<br></code></pre></td></tr></table></figure><p>程序运行结束后，可以在浏览器中输入“<a href="http://hadoop01:8088”查看集群信息，从中可以看到，“Name”这个列的值是“Flink">http://hadoop01:8088”查看集群信息，从中可以看到，“Name”这个列的值是“Flink</a> Application cluster”。</p><img src="./assets/image-20250930171331680.png" srcset="/img/loading.gif" lazyload alt="image-20250930171331680" style="zoom:80%"><p>把图4-31中的页面拉到最右侧，点击“ApplicationMaster”，就可以查看Flink集群运行程序的相关信息。</p><img src="./assets/image-20250930171418135.png" srcset="/img/loading.gif" lazyload alt="image-20250930171418135" style="zoom:80%"><h3 id="历史服务器"><a href="#历史服务器" class="headerlink" title="历史服务器"></a>历史服务器</h3><p>运行Flink作业的集群一旦停止，只能去YARN或者本地磁盘查看日志，无法通过Web页面查看相关信息。</p><p>如果有了历史服务器，就可以在Flink集群关闭后仍然可以查询作业运行信息。</p><p>在hadoop01节点上启动HDFS和YARN：</p><p>执行如下命令在HDFS中创建用于保存Flink作业运行信息的目录：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /usr/local/hadoop<br>./bin/hdfs dfs -<span class="hljs-built_in">mkdir</span> -p /logs/flink-job<br></code></pre></td></tr></table></figure><p>修改“&#x2F;usr&#x2F;local&#x2F;flink&#x2F;conf&#x2F;flink-conf.yaml”文件，按照如下内容设置相关配置项：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs stylus">jobmanager<span class="hljs-selector-class">.archive</span><span class="hljs-selector-class">.fs</span><span class="hljs-selector-class">.dir</span>: hdfs:<span class="hljs-comment">//hadoop01:9000/logs/flink-job</span><br>historyserver<span class="hljs-selector-class">.web</span><span class="hljs-selector-class">.address</span>: hadoop01<br>historyserver<span class="hljs-selector-class">.web</span><span class="hljs-selector-class">.port</span>: <span class="hljs-number">8082</span><br>historyserver<span class="hljs-selector-class">.archive</span><span class="hljs-selector-class">.fs</span><span class="hljs-selector-class">.dir</span>: hdfs:<span class="hljs-comment">//hadoop01:9000/logs/flink-job</span><br>historyserver<span class="hljs-selector-class">.archive</span><span class="hljs-selector-class">.fs</span><span class="hljs-selector-class">.refresh-interval</span>: <span class="hljs-number">5000</span><br></code></pre></td></tr></table></figure><p>执行如下命令启动历史服务器：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /usr/local/flink<br>./bin/historyserver.sh start<br></code></pre></td></tr></table></figure><p>使用JPS命令查询进程，可以看到多了一个名称为“HistoryServer”的进程。</p><p>在hadoop01节点上执行如下命令执行程序：</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs applescript">cd /usr/<span class="hljs-keyword">local</span>/flink<br>./bin/flink <span class="hljs-built_in">run</span>-<span class="hljs-built_in">application</span> \<br>-t yarn-<span class="hljs-built_in">application</span> \<br>-c cn.edu.pz.WordCount \<br>~/wordcount<span class="hljs-number">-1.0</span>.jar<br></code></pre></td></tr></table></figure><p>程序运行结束后，可以在浏览器中输入“<a href="http://hadoop01:8082”查看Flink集群历史作业信息，需要多次刷新页面，大概5分钟以后，可以看到在“Compeleted">http://hadoop01:8082”查看Flink集群历史作业信息，需要多次刷新页面，大概5分钟以后，可以看到在“Compeleted</a> Job List”下面会出现刚才已经运行结束的作业的信息。</p><img src="./assets/image-20250930171814060.png" srcset="/img/loading.gif" lazyload alt="image-20250930171814060" style="zoom:80%"><p>停止历史服务器就是stop啦。</p></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" class="category-chain-item">大数据</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/Flink/" class="print-no-link">#Flink</a></div></div><div class="license-box my-3"><div class="license-title"><div>文档汇总-Flink课堂部署</div><div>https://codesave666.github.io/2025/10/28/文档汇总-Flink课堂部署/</div></div><div class="license-meta"><div class="license-meta-item"><div>Beitragsautor</div><div>John Doe</div></div><div class="license-meta-item license-meta-date"><div>Veröffentlicht am</div><div>2025年10月28日</div></div><div class="license-meta-item"><div>Urheberrechtshinweis</div><div><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - Attribution"><i class="iconfont icon-cc-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"></article><article class="post-next col-6"></article></div></div></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>Inhaltsverzeichnis</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">Suchen</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">Stichwort</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",(function(){NProgress.done()}))</script><script src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t,e){var i=Fluid.plugins.typing,n=e.getElementById("subtitle");n&&i&&i(n.getAttribute("data-typed-text"))}(window,document)</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js",(function(){var t=jQuery("#toc");if(0!==t.length&&window.tocbot){var i=jQuery("#board-ctn").offset().top;window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-i},CONFIG.toc)),t.find(".toc-list-item").length>0&&t.css("visibility","visible"),Fluid.events.registerRefreshCallback((function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;t.find(".toc-list-item").length>0&&t.css("visibility","visible")}}))}}))</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js",(function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback((function(){if("anchors"in window){anchors.removeAll();var n=(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","),o=[];for(var s of n)o.push(".markdown-body > "+s.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}}))}))</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",(function(){Fluid.plugins.fancyBox()}))</script><script>Fluid.plugins.imageCaption()</script><script src="/js/local-search.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">Blog funktioniert am besten mit aktiviertem JavaScript</div></noscript></body></html>